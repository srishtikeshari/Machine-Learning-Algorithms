{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble\n",
    "\n",
    "\n",
    "Ensemble learning helps improve machine learning results by combining several models. This approach allows the production of better predictive performance compared to a single model.\n",
    "\n",
    "Most ensemble methods use a single base learning algorithm i.e. learners of the same type, leading to homogeneous ensembles.\n",
    "\n",
    "There are also some methods that use heterogeneous learners, i.e. learners of different types, leading to heterogeneous ensembles. In order for ensemble methods to be more accurate than any of its individual members, the base learners have to be as accurate as possible and as diverse as possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "\n",
    "Bagging stands for bootstrap aggregation. One way to reduce the variance of an estimate is to average together multiple estimates. For example, we can train M different trees on different subsets of the data (chosen randomly with replacement).\n",
    "\n",
    "Bagging uses bootstrap sampling to obtain the data subsets for training the base learners. For aggregating the outputs of base learners, bagging uses voting for classification and averaging for regression.\n",
    "\n",
    "### Boosting\n",
    "\n",
    "Boosting is a general ensemble method that creates a strong classifier from a number of weak classifiers.\n",
    "\n",
    "This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model. Models are added until the training set is predicted perfectly or a maximum number of models are added.\n",
    "\n",
    "AdaBoost is one of the most successful boosting algorithms developed for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries useful in Ensemble are listed below"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To load dataset:    import pandas as pd\n",
    "Preprocessing:      from sklearn import preprocessing\n",
    "Decision Tree:      from sklearn.tree import DecisionTreeClassifier \n",
    "K-NN Classifier:    from sklearn.neighbors import KNeighborsClassifier\n",
    "Naive Bayes:        from sklearn.naive_bayes import GaussianNB\n",
    "Train & Test Split: from sklearn.model_selection import train_test_split\n",
    "K-Fold:             from sklearn.model_selection import cross_val_score\n",
    "Bagging Algorithms: from sklearn.ensemble import BaggingClassifier\n",
    "Voting Classifier:  from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "For Prediction and Evaluation\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the libraries required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the \"letter-recognition\" data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16\n",
       "0  T   2   8   3   5   1   8  13   0   6   6  10   8   0   8   0   8\n",
       "1  I   5  12   3   7   2  10   5   5   4  13   3   9   2   8   4  10\n",
       "2  D   4  11   6   8   6  10   6   2   6  10   3   7   3   7   3   9\n",
       "3  N   7  11   6   6   3   5   9   4   6   4   4  10   6  10   2   8\n",
       "4  G   2   1   3   1   1   8   6   6   6   6   5   9   1   7   5  10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"letter-recognition.data.txt\", header = None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into training and testing parts (70-30 ratio with a random state value 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16\n",
       "0   2   8   3   5   1   8  13   0   6   6  10   8   0   8   0   8\n",
       "1   5  12   3   7   2  10   5   5   4  13   3   9   2   8   4  10\n",
       "2   4  11   6   8   6  10   6   2   6  10   3   7   3   7   3   9\n",
       "3   7  11   6   6   3   5   9   4   6   4   4  10   6  10   2   8\n",
       "4   2   1   3   1   1   8   6   6   6   6   5   9   1   7   5  10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the independent variables and the target attribute\n",
    "X = df[df.columns[1:]] # Selecting the independent variables\n",
    "Y = df[df.columns[0]] # selecting only the target lableled column\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the dataset into training and testing partition\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Ensemble Method by manipulation of Dataset (Bagged Decision Trees)\n",
    "\n",
    "Bagging performs best with algorithms that have high variance. A popular example are decision trees, often constructed without pruning.\n",
    "\n",
    "We will create decision tree classifiers with and without bagging ensemble method and compare their performance."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Implement the Decision Tree classifier\n",
    "2. Implement Bagging Classifier using base classifiers of the same type i.e. Decision tree (no. of classifiers    = 5)\n",
    "3. Train and view scores using k-fold cross validation with k=5\n",
    "4. Test on the testing part of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the decision tree classifier using entropy and random state value as 30\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree_entropy = DecisionTreeClassifier(criterion='entropy', random_state = 30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:  [0.865      0.86964286 0.85214286 0.86       0.87      ]\n",
      "mean score:  0.8633571428571427\n"
     ]
    }
   ],
   "source": [
    "# Use k-fold cross validation with k=5\n",
    "from sklearn.model_selection import cross_val_score\n",
    "dtree_entropy = dtree_entropy.fit(X_train,Y_train)\n",
    "scores = cross_val_score(dtree_entropy, X_train, Y_train, cv=5, scoring='accuracy')\n",
    "print('scores: ', scores)\n",
    "print('mean score: ', scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'L', 'O', ..., 'N', 'T', 'C'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict results on the testing part\n",
    "Y_pred = dtree_entropy.predict(X_test)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.93      0.91      0.92       229\n",
      "           B       0.82      0.83      0.83       228\n",
      "           C       0.91      0.88      0.89       220\n",
      "           D       0.78      0.86      0.82       219\n",
      "           E       0.84      0.87      0.85       232\n",
      "           F       0.83      0.77      0.80       225\n",
      "           G       0.87      0.80      0.83       234\n",
      "           H       0.74      0.79      0.76       206\n",
      "           I       0.88      0.92      0.90       236\n",
      "           J       0.90      0.89      0.90       209\n",
      "           K       0.83      0.84      0.84       213\n",
      "           L       0.92      0.92      0.92       239\n",
      "           M       0.91      0.91      0.91       240\n",
      "           N       0.89      0.87      0.88       239\n",
      "           O       0.90      0.81      0.85       243\n",
      "           P       0.85      0.92      0.88       243\n",
      "           Q       0.87      0.82      0.84       228\n",
      "           R       0.81      0.82      0.82       230\n",
      "           S       0.88      0.84      0.86       220\n",
      "           T       0.85      0.91      0.88       234\n",
      "           U       0.87      0.92      0.89       235\n",
      "           V       0.90      0.86      0.88       228\n",
      "           W       0.88      0.91      0.90       250\n",
      "           X       0.84      0.88      0.86       237\n",
      "           Y       0.85      0.87      0.86       252\n",
      "           Z       0.92      0.87      0.90       231\n",
      "\n",
      "    accuracy                           0.87      6000\n",
      "   macro avg       0.87      0.86      0.86      6000\n",
      "weighted avg       0.87      0.87      0.87      6000\n",
      "\n",
      "Confusion Matrix\n",
      "[[208   2   0   5   0   0   0   0   0   3   2   0   0   1   0   0   1   1\n",
      "    0   2   0   1   0   0   3   0]\n",
      " [  0 190   0   2   1   2   1   3   1   1   2   2   0   0   0   2   1   6\n",
      "    0   1   1   3   4   3   1   1]\n",
      " [  0   0 193   0   6   0   6   2   0   0   2   0   0   0   0   1   4   1\n",
      "    1   0   2   0   0   1   1   0]\n",
      " [  1   2   1 188   0   1   4   5   2   0   1   0   0   4   3   1   0   3\n",
      "    0   0   0   0   0   2   1   0]\n",
      " [  0   0   2   0 201   1   2   0   0   0   7   3   1   0   0   0   3   1\n",
      "    2   3   0   1   1   3   1   0]\n",
      " [  0   3   1   0   3 174   0   3   4   2   1   0   0   0   1  12   0   0\n",
      "    2  11   0   1   2   1   2   2]\n",
      " [  0   3   6   4   8   0 187   3   0   0   3   1   1   0   1   3   3   2\n",
      "    2   0   3   0   1   2   1   0]\n",
      " [  1   0   0  10   0   1   2 162   0   0   6   0   1   3   1   2   0  10\n",
      "    1   0   4   0   0   1   0   1]\n",
      " [  0   0   0   2   2   2   0   0 216   3   0   0   0   0   0   3   0   1\n",
      "    3   0   0   0   0   2   1   1]\n",
      " [  2   0   0   0   0   2   0   2   8 187   0   3   0   1   0   0   0   0\n",
      "    1   1   0   0   0   1   0   1]\n",
      " [  0   2   1   1   2   0   2   6   1   0 179   0   1   1   0   0   0   4\n",
      "    2   2   0   0   0   9   0   0]\n",
      " [  0   1   4   0   2   0   1   1   1   0   0 220   2   0   0   1   1   1\n",
      "    0   0   3   0   0   1   0   0]\n",
      " [  3   0   0   0   1   0   0   2   0   0   1   0 219   2   0   1   1   1\n",
      "    0   0   0   3   5   0   1   0]\n",
      " [  0   2   0   6   0   0   0   1   0   0   2   0   3 207   3   0   0   4\n",
      "    1   0   6   1   3   0   0   0]\n",
      " [  0   1   1   5   0   0   3   6   0   1   0   0   0   1 197   2   8   3\n",
      "    4   0   6   0   4   1   0   0]\n",
      " [  0   1   0   0   0   4   0   2   2   1   0   0   0   0   0 223   1   0\n",
      "    0   2   0   0   4   1   2   0]\n",
      " [  1   1   1   0   5   2   1   4   1   1   0   3   0   0   6   0 186   1\n",
      "    2   0   2   1   1   3   2   4]\n",
      " [  1  12   0   3   2   2   2   4   0   0   5   1   0   1   0   2   0 189\n",
      "    0   1   0   1   0   1   0   3]\n",
      " [  0   3   0   5   3   3   1   3   2   1   0   1   0   0   1   1   0   2\n",
      "  185   2   1   0   1   2   1   2]\n",
      " [  0   2   1   1   0   4   0   1   2   0   0   0   0   0   0   0   0   1\n",
      "    0 212   0   0   0   0  10   0]\n",
      " [  1   0   0   2   0   0   0   2   1   0   0   0   2   2   4   0   2   0\n",
      "    0   0 216   0   0   0   3   0]\n",
      " [  3   3   0   0   0   2   1   1   0   0   1   0   3   5   0   1   0   0\n",
      "    0   0   1 197   4   0   6   0]\n",
      " [  0   0   0   0   0   1   1   2   0   0   0   0   5   4   2   1   1   0\n",
      "    0   1   0   4 228   0   0   0]\n",
      " [  1   2   1   5   2   3   0   2   0   3   2   2   0   0   0   1   1   2\n",
      "    1   0   0   0   0 208   0   1]\n",
      " [  2   1   1   0   0   5   0   0   0   0   0   1   2   0   0   4   0   0\n",
      "    1   8   3   5   0   0 218   1]\n",
      " [  0   0   0   1   1   0   2   1   4   5   1   3   0   0   0   0   0   0\n",
      "    2   2   0   0   0   7   1 201]]\n",
      "\n",
      " Accuracy\n",
      "0.8651666666666666\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print confusion matrix and other performance measures \n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(classification_report(Y_test,Y_pred))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print(\"\\n Accuracy\")\n",
    "print(accuracy_score(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Bagged Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a moodel using bagging using 5 decision tree classifiers\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "seed = 30\n",
    "dtree = DecisionTreeClassifier(criterion='entropy', random_state = 30) \n",
    "num_trees = 5\n",
    "model = BaggingClassifier(base_estimator=dtree, n_estimators=num_trees, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:  [0.88714286 0.89035714 0.89       0.88285714 0.88714286]\n",
      "mean score:  0.8875\n"
     ]
    }
   ],
   "source": [
    "# Use k-fold cross validation with k=5\n",
    "scores_ensemble = cross_val_score(model, X_train, Y_train, cv=5, scoring='accuracy')\n",
    "print('scores: ', scores_ensemble)\n",
    "print('mean score: ', scores_ensemble.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict results on the testing part\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred_ensemble = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.93      0.98      0.95       229\n",
      "           B       0.78      0.92      0.84       228\n",
      "           C       0.88      0.89      0.89       220\n",
      "           D       0.78      0.90      0.84       219\n",
      "           E       0.84      0.91      0.87       232\n",
      "           F       0.87      0.81      0.84       225\n",
      "           G       0.85      0.82      0.84       234\n",
      "           H       0.82      0.86      0.84       206\n",
      "           I       0.90      0.93      0.91       236\n",
      "           J       0.93      0.89      0.91       209\n",
      "           K       0.87      0.92      0.89       213\n",
      "           L       0.94      0.92      0.93       239\n",
      "           M       0.93      0.93      0.93       240\n",
      "           N       0.96      0.90      0.92       239\n",
      "           O       0.87      0.81      0.84       243\n",
      "           P       0.89      0.93      0.91       243\n",
      "           Q       0.89      0.89      0.89       228\n",
      "           R       0.89      0.86      0.88       230\n",
      "           S       0.92      0.89      0.90       220\n",
      "           T       0.93      0.90      0.91       234\n",
      "           U       0.92      0.91      0.91       235\n",
      "           V       0.94      0.86      0.90       228\n",
      "           W       0.94      0.90      0.92       250\n",
      "           X       0.92      0.92      0.92       237\n",
      "           Y       0.92      0.87      0.89       252\n",
      "           Z       0.94      0.87      0.91       231\n",
      "\n",
      "    accuracy                           0.89      6000\n",
      "   macro avg       0.89      0.89      0.89      6000\n",
      "weighted avg       0.89      0.89      0.89      6000\n",
      "\n",
      "Confusion Matrix\n",
      "[[224   1   0   0   0   0   0   0   2   0   0   0   0   0   0   0   0   0\n",
      "    1   0   0   0   0   1   0   0]\n",
      " [  0 210   0   3   2   1   0   3   0   0   1   0   1   0   0   0   0   0\n",
      "    1   0   2   0   0   2   0   2]\n",
      " [  0   0 196   2   7   0   7   0   1   0   1   1   0   0   2   0   1   1\n",
      "    0   0   0   0   1   0   0   0]\n",
      " [  0   2   0 198   0   1   3   4   4   0   0   0   0   1   3   0   1   1\n",
      "    0   0   0   0   0   0   1   0]\n",
      " [  0   0   4   1 211   0   4   0   0   0   1   0   0   0   0   0   0   0\n",
      "    2   1   0   0   0   5   1   2]\n",
      " [  0   5   1   3   2 182   1   1   1   0   0   0   1   0   0  13   1   1\n",
      "    1   6   0   0   1   0   4   1]\n",
      " [  3   6   3   4   7   0 193   0   0   1   4   1   0   0   3   0   5   1\n",
      "    1   0   0   1   0   1   0   0]\n",
      " [  1   4   0   6   0   1   1 177   0   0   4   0   1   1   1   1   0   4\n",
      "    2   0   2   0   0   0   0   0]\n",
      " [  0   1   2   1   0   1   1   0 219   4   1   1   0   0   0   2   0   1\n",
      "    1   0   0   0   0   0   0   1]\n",
      " [  0   1   1   2   1   3   0   3   9 185   0   2   0   0   0   0   0   0\n",
      "    0   0   2   0   0   0   0   0]\n",
      " [  0   2   0   1   4   0   0   1   2   0 195   0   0   1   0   0   0   5\n",
      "    0   0   0   0   0   2   0   0]\n",
      " [  0   2   0   0   3   1   7   2   0   0   1 221   0   1   0   1   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  2   0   1   0   2   0   1   2   0   0   1   0 223   1   0   1   0   0\n",
      "    0   0   2   1   2   0   1   0]\n",
      " [  0   0   1   5   0   0   0   2   0   1   3   0   3 214   2   0   0   2\n",
      "    0   0   2   1   2   0   1   0]\n",
      " [  0   5   3  13   0   0   3   0   0   1   3   0   1   0 196   0  10   1\n",
      "    0   1   1   1   3   1   0   0]\n",
      " [  0   0   0   3   0   7   0   3   0   0   0   0   0   0   1 225   1   0\n",
      "    0   0   0   1   0   2   0   0]\n",
      " [  2   1   2   2   2   0   0   2   1   0   1   0   1   0   6   0 204   2\n",
      "    0   1   0   0   0   0   0   1]\n",
      " [  4   8   1   2   2   1   0   4   0   0   4   1   0   2   0   1   0 198\n",
      "    1   0   0   1   0   0   0   0]\n",
      " [  0   6   2   0   3   2   0   0   1   1   1   0   0   0   1   1   2   1\n",
      "  196   0   0   0   0   1   1   1]\n",
      " [  3   2   1   1   1   3   2   0   0   0   0   1   0   0   1   0   0   0\n",
      "    1 210   0   1   0   1   5   1]\n",
      " [  2   1   1   3   0   0   1   4   1   1   0   0   4   1   2   0   0   0\n",
      "    0   0 214   0   0   0   0   0]\n",
      " [  0  10   0   0   0   1   0   1   0   0   0   0   1   2   1   3   0   0\n",
      "    0   0   2 197   4   0   6   0]\n",
      " [  1   0   0   1   1   1   0   0   1   0   0   0   4   0   6   0   1   1\n",
      "    0   1   5   1 226   0   0   0]\n",
      " [  0   0   1   2   1   1   0   3   0   2   2   1   0   0   0   1   0   2\n",
      "    2   0   0   0   0 217   0   2]\n",
      " [  0   1   2   1   0   3   1   3   0   0   0   1   1   0   0   3   1   0\n",
      "    1   7   1   4   2   0 219   1]\n",
      " [  0   2   0   1   3   1   1   0   2   3   0   4   0   0   0   0   3   1\n",
      "    4   0   0   0   0   4   0 202]]\n",
      "\n",
      " Accuracy\n",
      "0.892\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print confusion matrix and other performance measures \n",
    "print(classification_report(Y_test,Y_pred_ensemble))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(Y_test,Y_pred_ensemble))\n",
    "print(\"\\n Accuracy\")\n",
    "print(accuracy_score(Y_test,Y_pred_ensemble))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Ensemble Method by manipulation of Classifiers (using Voting Classifier)\n",
    "\n",
    "The VotingClassifier takes in a list of different estimators as arguments and a voting method. The **hard** voting method uses the predicted labels and a majority rules system, while the **soft** voting method predicts a label based on the argmax/largest predicted value of the sum of the predicted probabilities.\n",
    "\n",
    "After we provide the desired classifiers, we need to fit the resulting ensemble classifier object. We can then get predictions and use accuracy metrics."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Implement different base classifiers. Classifiers to be used are: Decision tree, 3-NN with euclidean \n",
    "   distance, 5-NN with euclidean distance, 5-NN with manhattan distance and \n",
    "   Naive Bayes\n",
    "2. Create an ensemble of the classifiers using Voting Classifier with hard voting method\n",
    "3. Test on the testing part of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required library\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the different classifiers\n",
    "dt = DecisionTreeClassifier(criterion='gini', random_state = 30)\n",
    "knn1 = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "knn2 = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "knn3 = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
    "nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Voting Classifier using above estimators and hard voting method\n",
    "# Function to be used: VotingClassifier(estimators,voting)\n",
    "# Estimators represent the base classifiers used taken as ('base classifier name', variable_name)\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "model_voting = VotingClassifier(estimators=[('m1_dt', dt),('m2_knn1', knn1),('m3_knn2',knn2),('m4_knn3',knn3),('m5_nb',nb)],voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:  [0.94178571 0.945      0.94357143 0.94464286 0.94357143]\n",
      "mean score:  0.9437142857142857\n"
     ]
    }
   ],
   "source": [
    "# Fit the voting classifier model and print scores using k-fold cross validation with k=5\n",
    "model_voting.fit(X_train, Y_train)\n",
    "scores_voting = cross_val_score(model_voting, X_train, Y_train, cv=5, scoring='accuracy')\n",
    "print('scores: ', scores_voting)\n",
    "print('mean score: ', scores_voting.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict results on the testing part\n",
    "Y_pred_voting = model_voting.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.98      1.00      0.99       229\n",
      "           B       0.84      0.98      0.90       228\n",
      "           C       0.96      0.95      0.96       220\n",
      "           D       0.89      0.98      0.93       219\n",
      "           E       0.94      0.93      0.93       232\n",
      "           F       0.93      0.93      0.93       225\n",
      "           G       0.94      0.91      0.93       234\n",
      "           H       0.89      0.92      0.90       206\n",
      "           I       0.94      0.97      0.96       236\n",
      "           J       0.97      0.93      0.95       209\n",
      "           K       0.93      0.91      0.92       213\n",
      "           L       0.99      0.95      0.97       239\n",
      "           M       0.97      0.98      0.98       240\n",
      "           N       0.98      0.95      0.96       239\n",
      "           O       0.92      0.95      0.93       243\n",
      "           P       0.95      0.93      0.94       243\n",
      "           Q       0.97      0.96      0.97       228\n",
      "           R       0.94      0.91      0.93       230\n",
      "           S       0.99      0.98      0.98       220\n",
      "           T       0.97      0.96      0.96       234\n",
      "           U       1.00      0.97      0.98       235\n",
      "           V       0.96      0.93      0.95       228\n",
      "           W       0.97      0.95      0.96       250\n",
      "           X       0.96      0.96      0.96       237\n",
      "           Y       0.98      0.95      0.96       252\n",
      "           Z       0.99      0.97      0.98       231\n",
      "\n",
      "    accuracy                           0.95      6000\n",
      "   macro avg       0.95      0.95      0.95      6000\n",
      "weighted avg       0.95      0.95      0.95      6000\n",
      "\n",
      "Confusion Matrix\n",
      "[[228   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0 223   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   2\n",
      "    0   0   0   0   0   2   0   0]\n",
      " [  0   0 210   0   1   0   3   0   0   0   0   0   0   0   3   0   0   0\n",
      "    0   0   0   1   1   1   0   0]\n",
      " [  0   1   0 214   0   0   0   1   0   0   0   0   0   0   1   1   0   1\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   2   2   0 215   1   3   1   0   0   1   0   0   0   0   0   0   0\n",
      "    2   0   0   1   0   1   0   3]\n",
      " [  0   1   0   1   0 210   0   0   1   0   0   0   0   1   0   7   0   0\n",
      "    0   2   0   1   1   0   0   0]\n",
      " [  0   2   1   3   4   0 214   0   0   0   1   0   0   0   5   0   1   1\n",
      "    0   0   0   0   1   1   0   0]\n",
      " [  0   5   0   4   0   0   1 189   0   0   3   0   0   0   0   0   0   4\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   1   0   0   0   0   0 230   4   0   0   0   0   0   1   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   0  12 195   0   0   0   1   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   4   0   1   1   0   0   9   0   0 193   0   0   0   0   0   0   2\n",
      "    0   0   0   0   0   3   0   0]\n",
      " [  0   1   0   0   2   0   4   4   0   0   1 227   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   2   0   0   0   0   0   0   0   0   0   0 236   0   0   0   0   0\n",
      "    0   0   0   0   2   0   0   0]\n",
      " [  0   1   0   5   0   0   0   1   0   0   0   0   1 226   2   0   0   2\n",
      "    0   0   0   1   0   0   0   0]\n",
      " [  0   3   3   4   0   0   1   0   0   0   0   0   0   0 230   0   2   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   1   0   0  14   0   0   0   0   0   0   0   0   0 227   0   1\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  2   0   0   1   0   0   1   0   0   0   0   0   0   0   5   0 219   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   9   0   3   0   0   0   2   0   0   2   1   0   3   0   0   0 210\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   1   0   1   1   1   0   0   0   0   1   0   0   0   0   0   0   0\n",
      "  215   0   0   0   0   0   0   0]\n",
      " [  0   1   0   0   0   0   0   1   1   0   1   1   0   0   0   0   0   0\n",
      "    0 224   0   0   0   0   5   0]\n",
      " [  1   0   0   1   0   0   0   4   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0 229   0   0   0   0   0]\n",
      " [  1   9   0   0   0   0   0   0   0   0   0   0   2   0   0   0   0   0\n",
      "    0   0   0 213   2   0   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   4   0   5   0   0   1\n",
      "    0   0   1   1 238   0   0   0]\n",
      " [  0   1   0   2   2   0   0   0   0   0   4   0   0   0   0   0   0   0\n",
      "    1   0   0   0   0 227   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   2   0   0\n",
      "    0   4   0   4   1   0 240   0]\n",
      " [  0   0   0   0   1   0   0   0   0   1   0   0   0   0   0   0   3   0\n",
      "    0   2   0   0   0   1   0 223]]\n",
      "\n",
      " Accuracy\n",
      "0.9508333333333333\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print confusion matrix and other performance measures \n",
    "print(classification_report(Y_test,Y_pred_voting))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(Y_test,Y_pred_voting))\n",
    "print(\"\\n Accuracy\")\n",
    "print(accuracy_score(Y_test,Y_pred_voting))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Manipulating the features"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Generate five feature sets using random vector (including 10 features each)\n",
    "2. Apply Decision tree with same hyperparameters on these five different feature \n",
    "   sets\n",
    "3. Test on the testing part of the dataset\n",
    "(Note: The ensemble classifier would be evaluated on the entire dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate five random vectors\n",
    "df1 = df.copy(deep=True)\n",
    "df2 = df.copy(deep=True)\n",
    "df3 = df.copy(deep=True)\n",
    "df4 = df.copy(deep=True)\n",
    "df5 = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  3  4  5  7  8 11 12 13 15]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', random_state=30)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 1\n",
    "# Select the independent variables \n",
    "# select only the target lableled column\n",
    "# Train the model\n",
    "x1 = np.random.choice(np.arange(1,17),10,replace=False)\n",
    "x1.sort()\n",
    "print(x1)\n",
    "X1 = df1[df1.columns[x1]]\n",
    "Y1 = df1[0]\n",
    "X1.head() \n",
    "X1_train, X1_test, Y1_train, Y1_test = train_test_split(X1, Y1, test_size=0.30, random_state = 30)\n",
    "dt1 = DecisionTreeClassifier(criterion='entropy', random_state = 30)\n",
    "dt1.fit(X1_train, Y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  3  4  5  6  7 12 13 14 15]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', random_state=30)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 2\n",
    "# Select the independent variables \n",
    "# select only the target lableled column\n",
    "# Train the model\n",
    "x2 = np.random.choice(np.arange(1,17),10,replace=False)\n",
    "x2.sort()\n",
    "print(x2)\n",
    "X2 = df2[df2.columns[x2]]\n",
    "Y2 = df2[0]\n",
    "X2.head() \n",
    "X2_train, X2_test, Y2_train, Y2_test = train_test_split(X2, Y2, test_size=0.30, random_state = 30)\n",
    "dt2 = DecisionTreeClassifier(criterion='entropy', random_state = 30)\n",
    "dt2.fit(X2_train, Y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  4  5  6  8  9 10 11 13 15]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', random_state=30)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 3\n",
    "# Select the independent variables \n",
    "# select only the target lableled column\n",
    "# Train the model\n",
    "x3 = np.random.choice(np.arange(1,17),10,replace=False)\n",
    "x3.sort()\n",
    "print(x3)\n",
    "X3 = df3[df3.columns[x3]]\n",
    "Y3 = df3[0]\n",
    "X3.head() \n",
    "X3_train, X3_test, Y3_train, Y3_test = train_test_split(X3, Y3, test_size=0.30, random_state = 30)\n",
    "dt3 = DecisionTreeClassifier(criterion='entropy', random_state = 30)\n",
    "dt3.fit(X3_train, Y3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  4  5  7  9 11 12 13 14 15]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', random_state=30)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 4\n",
    "# Select the independent variables \n",
    "# select only the target lableled column\n",
    "# Train the model\n",
    "x4 = np.random.choice(np.arange(1,17),10,replace=False)\n",
    "x4.sort()\n",
    "print(x4)\n",
    "X4 = df4[df4.columns[x4]]\n",
    "Y4 = df4[0]\n",
    "X4.head() \n",
    "X4_train, X4_test, Y4_train, Y4_test = train_test_split(X4, Y4, test_size=0.30, random_state = 30)\n",
    "dt4 = DecisionTreeClassifier(criterion='entropy', random_state = 30)\n",
    "dt4.fit(X4_train, Y4_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  3  5  6  8 10 11 12 14 15]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', random_state=30)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 5\n",
    "# Select the independent variables \n",
    "# select only the target lableled column\n",
    "# Train the model\n",
    "x5 = np.random.choice(np.arange(1,17),10,replace=False)\n",
    "x5.sort()\n",
    "print(x5)\n",
    "X5 = df5[df5.columns[x5]]\n",
    "Y5 = df5[0]\n",
    "X5.head() \n",
    "X5_train, X5_test, Y5_train, Y5_test = train_test_split(X5, Y5, test_size=0.30, random_state = 30)\n",
    "dt5 = DecisionTreeClassifier(criterion='entropy', random_state = 30)\n",
    "dt5.fit(X5_train, Y5_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'L', 'O', ..., 'N', 'T', 'C'], dtype=object)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply Voting Classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "model_feature_subset = VotingClassifier(estimators=[('m1', dt1),('m2', dt2),('m3',dt3),('m4',dt4),('m5',dt5)],voting='hard')\n",
    "model_feature_subset.fit(X_train, Y_train)\n",
    "Y_pred_feature_subset = model_feature_subset.predict(X_test)\n",
    "Y_pred_feature_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.93      0.91      0.92       229\n",
      "           B       0.82      0.83      0.83       228\n",
      "           C       0.91      0.88      0.89       220\n",
      "           D       0.78      0.86      0.82       219\n",
      "           E       0.84      0.87      0.85       232\n",
      "           F       0.83      0.77      0.80       225\n",
      "           G       0.87      0.80      0.83       234\n",
      "           H       0.74      0.79      0.76       206\n",
      "           I       0.88      0.92      0.90       236\n",
      "           J       0.90      0.89      0.90       209\n",
      "           K       0.83      0.84      0.84       213\n",
      "           L       0.92      0.92      0.92       239\n",
      "           M       0.91      0.91      0.91       240\n",
      "           N       0.89      0.87      0.88       239\n",
      "           O       0.90      0.81      0.85       243\n",
      "           P       0.85      0.92      0.88       243\n",
      "           Q       0.87      0.82      0.84       228\n",
      "           R       0.81      0.82      0.82       230\n",
      "           S       0.88      0.84      0.86       220\n",
      "           T       0.85      0.91      0.88       234\n",
      "           U       0.87      0.92      0.89       235\n",
      "           V       0.90      0.86      0.88       228\n",
      "           W       0.88      0.91      0.90       250\n",
      "           X       0.84      0.88      0.86       237\n",
      "           Y       0.85      0.87      0.86       252\n",
      "           Z       0.92      0.87      0.90       231\n",
      "\n",
      "    accuracy                           0.87      6000\n",
      "   macro avg       0.87      0.86      0.86      6000\n",
      "weighted avg       0.87      0.87      0.87      6000\n",
      "\n",
      "Confusion Matrix\n",
      "[[208   2   0   5   0   0   0   0   0   3   2   0   0   1   0   0   1   1\n",
      "    0   2   0   1   0   0   3   0]\n",
      " [  0 190   0   2   1   2   1   3   1   1   2   2   0   0   0   2   1   6\n",
      "    0   1   1   3   4   3   1   1]\n",
      " [  0   0 193   0   6   0   6   2   0   0   2   0   0   0   0   1   4   1\n",
      "    1   0   2   0   0   1   1   0]\n",
      " [  1   2   1 188   0   1   4   5   2   0   1   0   0   4   3   1   0   3\n",
      "    0   0   0   0   0   2   1   0]\n",
      " [  0   0   2   0 201   1   2   0   0   0   7   3   1   0   0   0   3   1\n",
      "    2   3   0   1   1   3   1   0]\n",
      " [  0   3   1   0   3 174   0   3   4   2   1   0   0   0   1  12   0   0\n",
      "    2  11   0   1   2   1   2   2]\n",
      " [  0   3   6   4   8   0 187   3   0   0   3   1   1   0   1   3   3   2\n",
      "    2   0   3   0   1   2   1   0]\n",
      " [  1   0   0  10   0   1   2 162   0   0   6   0   1   3   1   2   0  10\n",
      "    1   0   4   0   0   1   0   1]\n",
      " [  0   0   0   2   2   2   0   0 216   3   0   0   0   0   0   3   0   1\n",
      "    3   0   0   0   0   2   1   1]\n",
      " [  2   0   0   0   0   2   0   2   8 187   0   3   0   1   0   0   0   0\n",
      "    1   1   0   0   0   1   0   1]\n",
      " [  0   2   1   1   2   0   2   6   1   0 179   0   1   1   0   0   0   4\n",
      "    2   2   0   0   0   9   0   0]\n",
      " [  0   1   4   0   2   0   1   1   1   0   0 220   2   0   0   1   1   1\n",
      "    0   0   3   0   0   1   0   0]\n",
      " [  3   0   0   0   1   0   0   2   0   0   1   0 219   2   0   1   1   1\n",
      "    0   0   0   3   5   0   1   0]\n",
      " [  0   2   0   6   0   0   0   1   0   0   2   0   3 207   3   0   0   4\n",
      "    1   0   6   1   3   0   0   0]\n",
      " [  0   1   1   5   0   0   3   6   0   1   0   0   0   1 197   2   8   3\n",
      "    4   0   6   0   4   1   0   0]\n",
      " [  0   1   0   0   0   4   0   2   2   1   0   0   0   0   0 223   1   0\n",
      "    0   2   0   0   4   1   2   0]\n",
      " [  1   1   1   0   5   2   1   4   1   1   0   3   0   0   6   0 186   1\n",
      "    2   0   2   1   1   3   2   4]\n",
      " [  1  12   0   3   2   2   2   4   0   0   5   1   0   1   0   2   0 189\n",
      "    0   1   0   1   0   1   0   3]\n",
      " [  0   3   0   5   3   3   1   3   2   1   0   1   0   0   1   1   0   2\n",
      "  185   2   1   0   1   2   1   2]\n",
      " [  0   2   1   1   0   4   0   1   2   0   0   0   0   0   0   0   0   1\n",
      "    0 212   0   0   0   0  10   0]\n",
      " [  1   0   0   2   0   0   0   2   1   0   0   0   2   2   4   0   2   0\n",
      "    0   0 216   0   0   0   3   0]\n",
      " [  3   3   0   0   0   2   1   1   0   0   1   0   3   5   0   1   0   0\n",
      "    0   0   1 197   4   0   6   0]\n",
      " [  0   0   0   0   0   1   1   2   0   0   0   0   5   4   2   1   1   0\n",
      "    0   1   0   4 228   0   0   0]\n",
      " [  1   2   1   5   2   3   0   2   0   3   2   2   0   0   0   1   1   2\n",
      "    1   0   0   0   0 208   0   1]\n",
      " [  2   1   1   0   0   5   0   0   0   0   0   1   2   0   0   4   0   0\n",
      "    1   8   3   5   0   0 218   1]\n",
      " [  0   0   0   1   1   0   2   1   4   5   1   3   0   0   0   0   0   0\n",
      "    2   2   0   0   0   7   1 201]]\n",
      "\n",
      " Accuracy\n",
      "0.8651666666666666\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print confusion matrix and other performance measures \n",
    "print(classification_report(Y_test,Y_pred_feature_subset))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(Y_test,Y_pred_feature_subset))\n",
    "print(\"\\n Accuracy\")\n",
    "print(accuracy_score(Y_test,Y_pred_feature_subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. Manipulating the classes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Convert the problem into two-class problem. Create 5 sets of two classes \n",
    "   using random vector \n",
    "2. Apply five Decision tree with same hyperparametrs on these five sets.\n",
    "3. Test on the testing part of the dataset\n",
    "(Note: The ensemble classifier would be evaluated on the entire dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6. 18. 17. 16. 13. 25. 14. 12.  4. 19. 10. 11. 22.]\n",
      " [24.  5. 15.  9.  2.  1. 20. 21.  4. 10. 12.  3. 18.]\n",
      " [ 3. 17. 20. 11.  6.  9.  1. 25. 13.  7. 18. 26. 23.]\n",
      " [18.  5. 12.  8.  6. 13.  4.  1. 14. 26. 19. 21. 17.]\n",
      " [ 1. 12.  6. 22.  4. 10. 24. 18.  3. 13.  8.  9. 19.]]\n"
     ]
    }
   ],
   "source": [
    "# Generate 5 sets of two class representation\n",
    "features = np.zeros((5,13))\n",
    "for i in range(5):\n",
    "    features[i] = np.random.choice(np.arange(1,27),13,replace=False)\n",
    "    \n",
    "print(features)\n",
    "df1 = df.copy(deep = True)\n",
    "df2 = df.copy(deep = True)\n",
    "df3 = df.copy(deep = True)\n",
    "df4 = df.copy(deep = True)\n",
    "df5 = df.copy(deep = True)\n",
    "\n",
    "for i in range(len(df)):\n",
    "    col = ord(df.iloc[i, 0]) - 64\n",
    "    \n",
    "    if col not in features[0]:\n",
    "        df1.iloc[i, 0] = \"1\"\n",
    "    else:\n",
    "        df1.iloc[i, 0] = \"0\"\n",
    "        \n",
    "    if col not in features[1]:\n",
    "        df2.iloc[i, 0] = \"1\"\n",
    "    else:\n",
    "        df2.iloc[i, 0] = \"0\"\n",
    "        \n",
    "    if col not in features[2]:\n",
    "        df3.iloc[i, 0] = \"1\"\n",
    "    else:\n",
    "        df3.iloc[i, 0] = \"0\"\n",
    "        \n",
    "    if col not in features[3]:\n",
    "        df4.iloc[i, 0] = \"1\"\n",
    "    else:\n",
    "        df4.iloc[i, 0] = \"0\"\n",
    "        \n",
    "    if col not in features[4]:\n",
    "        df5.iloc[i, 0] = \"1\"\n",
    "    else:\n",
    "        df5.iloc[i, 0] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1\n",
    "# Select the independent variables \n",
    "# select only the target lableled column\n",
    "# Train the model\n",
    "X1 = df1[df1.columns[1:]]\n",
    "Y1 = df1[df1.columns[0]]\n",
    "X1_train, X1_test, Y1_train, Y1_test = train_test_split(X1, Y1, test_size=0.30, random_state = 30)\n",
    "dt1 = DecisionTreeClassifier(criterion='entropy', random_state = 30)\n",
    "dt1 = dt1.fit(X1_train, Y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2\n",
    "# Select the independent variables \n",
    "# select only the target lableled column\n",
    "# Train the model\n",
    "X2 = df2[df2.columns[1:]]\n",
    "Y2 = df2[df2.columns[0]]\n",
    "X2_train, X2_test, Y2_train, Y2_test = train_test_split(X2, Y2, test_size=0.30, random_state = 30)\n",
    "dt2 = DecisionTreeClassifier(criterion='entropy', random_state = 30)\n",
    "dt2 = dt2.fit(X2_train, Y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3\n",
    "# Select the independent variables \n",
    "# select only the target lableled column\n",
    "# Train the model\n",
    "X3 = df1[df3.columns[1:]]\n",
    "Y3 = df1[df3.columns[0]]\n",
    "X3_train, X3_test, Y3_train, Y3_test = train_test_split(X3, Y3, test_size=0.30, random_state = 30)\n",
    "dt3 = DecisionTreeClassifier(criterion='entropy', random_state = 30)\n",
    "dt3 = dt3.fit(X3_train, Y3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4\n",
    "# Select the independent variables \n",
    "# select only the target lableled column\n",
    "# Train the model\n",
    "X4 = df4[df4.columns[1:]]\n",
    "Y4 = df4[df4.columns[0]]\n",
    "X4_train, X4_test, Y4_train, Y4_test = train_test_split(X4, Y4, test_size=0.30, random_state = 30)\n",
    "dt4 = DecisionTreeClassifier(criterion='entropy', random_state = 30)\n",
    "dt4 = dt4.fit(X4_train, Y4_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5\n",
    "# Select the independent variables \n",
    "# select only the target lableled column\n",
    "# Train the model\n",
    "X5 = df5[df5.columns[1:]]\n",
    "Y5 = df5[df5.columns[0]]\n",
    "X5_train, X5_test, Y5_train, Y5_test = train_test_split(X5, Y5, test_size=0.30, random_state = 30)\n",
    "dt5 = DecisionTreeClassifier(criterion='entropy', random_state = 30)\n",
    "dt5 = dt1.fit(X5_train, Y5_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:  [0.865      0.86964286 0.85214286 0.86       0.87      ]\n",
      "mean score:  0.8633571428571427\n"
     ]
    }
   ],
   "source": [
    "# Apply Voting Classifier\n",
    "model_two_class = VotingClassifier(estimators=[('m1', dt1),('m2', dt2),('m3',dt3),('m4',dt4),('m5',dt5)],voting='hard')\n",
    "model_two_class.fit(X_train, Y_train)\n",
    "scores_two_class = cross_val_score(model_two_class, X_train, Y_train, cv=5, scoring='accuracy')\n",
    "print('scores: ', scores_two_class)\n",
    "print('mean score: ', scores_two_class.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.93      0.91      0.92       229\n",
      "           B       0.82      0.83      0.83       228\n",
      "           C       0.91      0.88      0.89       220\n",
      "           D       0.78      0.86      0.82       219\n",
      "           E       0.84      0.87      0.85       232\n",
      "           F       0.83      0.77      0.80       225\n",
      "           G       0.87      0.80      0.83       234\n",
      "           H       0.74      0.79      0.76       206\n",
      "           I       0.88      0.92      0.90       236\n",
      "           J       0.90      0.89      0.90       209\n",
      "           K       0.83      0.84      0.84       213\n",
      "           L       0.92      0.92      0.92       239\n",
      "           M       0.91      0.91      0.91       240\n",
      "           N       0.89      0.87      0.88       239\n",
      "           O       0.90      0.81      0.85       243\n",
      "           P       0.85      0.92      0.88       243\n",
      "           Q       0.87      0.82      0.84       228\n",
      "           R       0.81      0.82      0.82       230\n",
      "           S       0.88      0.84      0.86       220\n",
      "           T       0.85      0.91      0.88       234\n",
      "           U       0.87      0.92      0.89       235\n",
      "           V       0.90      0.86      0.88       228\n",
      "           W       0.88      0.91      0.90       250\n",
      "           X       0.84      0.88      0.86       237\n",
      "           Y       0.85      0.87      0.86       252\n",
      "           Z       0.92      0.87      0.90       231\n",
      "\n",
      "    accuracy                           0.87      6000\n",
      "   macro avg       0.87      0.86      0.86      6000\n",
      "weighted avg       0.87      0.87      0.87      6000\n",
      "\n",
      "Confusion Matrix\n",
      "[[208   2   0   5   0   0   0   0   0   3   2   0   0   1   0   0   1   1\n",
      "    0   2   0   1   0   0   3   0]\n",
      " [  0 190   0   2   1   2   1   3   1   1   2   2   0   0   0   2   1   6\n",
      "    0   1   1   3   4   3   1   1]\n",
      " [  0   0 193   0   6   0   6   2   0   0   2   0   0   0   0   1   4   1\n",
      "    1   0   2   0   0   1   1   0]\n",
      " [  1   2   1 188   0   1   4   5   2   0   1   0   0   4   3   1   0   3\n",
      "    0   0   0   0   0   2   1   0]\n",
      " [  0   0   2   0 201   1   2   0   0   0   7   3   1   0   0   0   3   1\n",
      "    2   3   0   1   1   3   1   0]\n",
      " [  0   3   1   0   3 174   0   3   4   2   1   0   0   0   1  12   0   0\n",
      "    2  11   0   1   2   1   2   2]\n",
      " [  0   3   6   4   8   0 187   3   0   0   3   1   1   0   1   3   3   2\n",
      "    2   0   3   0   1   2   1   0]\n",
      " [  1   0   0  10   0   1   2 162   0   0   6   0   1   3   1   2   0  10\n",
      "    1   0   4   0   0   1   0   1]\n",
      " [  0   0   0   2   2   2   0   0 216   3   0   0   0   0   0   3   0   1\n",
      "    3   0   0   0   0   2   1   1]\n",
      " [  2   0   0   0   0   2   0   2   8 187   0   3   0   1   0   0   0   0\n",
      "    1   1   0   0   0   1   0   1]\n",
      " [  0   2   1   1   2   0   2   6   1   0 179   0   1   1   0   0   0   4\n",
      "    2   2   0   0   0   9   0   0]\n",
      " [  0   1   4   0   2   0   1   1   1   0   0 220   2   0   0   1   1   1\n",
      "    0   0   3   0   0   1   0   0]\n",
      " [  3   0   0   0   1   0   0   2   0   0   1   0 219   2   0   1   1   1\n",
      "    0   0   0   3   5   0   1   0]\n",
      " [  0   2   0   6   0   0   0   1   0   0   2   0   3 207   3   0   0   4\n",
      "    1   0   6   1   3   0   0   0]\n",
      " [  0   1   1   5   0   0   3   6   0   1   0   0   0   1 197   2   8   3\n",
      "    4   0   6   0   4   1   0   0]\n",
      " [  0   1   0   0   0   4   0   2   2   1   0   0   0   0   0 223   1   0\n",
      "    0   2   0   0   4   1   2   0]\n",
      " [  1   1   1   0   5   2   1   4   1   1   0   3   0   0   6   0 186   1\n",
      "    2   0   2   1   1   3   2   4]\n",
      " [  1  12   0   3   2   2   2   4   0   0   5   1   0   1   0   2   0 189\n",
      "    0   1   0   1   0   1   0   3]\n",
      " [  0   3   0   5   3   3   1   3   2   1   0   1   0   0   1   1   0   2\n",
      "  185   2   1   0   1   2   1   2]\n",
      " [  0   2   1   1   0   4   0   1   2   0   0   0   0   0   0   0   0   1\n",
      "    0 212   0   0   0   0  10   0]\n",
      " [  1   0   0   2   0   0   0   2   1   0   0   0   2   2   4   0   2   0\n",
      "    0   0 216   0   0   0   3   0]\n",
      " [  3   3   0   0   0   2   1   1   0   0   1   0   3   5   0   1   0   0\n",
      "    0   0   1 197   4   0   6   0]\n",
      " [  0   0   0   0   0   1   1   2   0   0   0   0   5   4   2   1   1   0\n",
      "    0   1   0   4 228   0   0   0]\n",
      " [  1   2   1   5   2   3   0   2   0   3   2   2   0   0   0   1   1   2\n",
      "    1   0   0   0   0 208   0   1]\n",
      " [  2   1   1   0   0   5   0   0   0   0   0   1   2   0   0   4   0   0\n",
      "    1   8   3   5   0   0 218   1]\n",
      " [  0   0   0   1   1   0   2   1   4   5   1   3   0   0   0   0   0   0\n",
      "    2   2   0   0   0   7   1 201]]\n",
      "\n",
      " Accuracy\n",
      "0.8651666666666666\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print confusion matrix and other performance measures \n",
    "Y_pred_two_class = model_two_class.predict(X_test)\n",
    "print(classification_report(Y_test,Y_pred_two_class))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(Y_test,Y_pred_two_class))\n",
    "print(\"\\n Accuracy\")\n",
    "print(accuracy_score(Y_test,Y_pred_two_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Which method performs the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
